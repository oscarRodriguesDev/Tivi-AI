'use client';


/**
 * Importa√ß√µes necess√°rias para o funcionamento do componente de videochamada:
 *
 * - `useEffect`, `useRef`, `useState`: Hooks do React para lidar com estado, refer√™ncias e efeitos colaterais.
 * - `useParams`: Hook do Next.js para acessar os par√¢metros da URL.
 * - `Peer`, `MediaConnection`: Componentes da biblioteca PeerJS para estabelecer conex√µes P2P de √°udio/v√≠deo.
 * - `LiveTranscription`: Componente personalizado respons√°vel pela transcri√ß√£o ao vivo da conversa.
 * - √çcones (`Mic`, `MicOff`, `Video`, `VideoOff`, `LogOut`): √çcones da biblioteca Lucide para representar os controles da interface.
 */

import { useEffect, useRef, useState } from "react";
import { useParams } from "next/navigation";
import Peer, { MediaConnection } from "peerjs";
import LiveTranscription from '../../components/transcriptPAC'
import { Mic, MicOff, Video, VideoOff, LogOut } from "lucide-react";


/**
 * Componente `PublicCallPage`
 *
 * Este componente representa a interface da videochamada p√∫blica entre psic√≥logo e paciente.
 * Ele utiliza a biblioteca `peerjs` para estabelecer uma conex√£o P2P (peer-to-peer) de v√≠deo e √°udio,
 * al√©m de monitorar o microfone do paciente e gerar transcri√ß√µes em tempo real, que s√£o exibidas para o psic√≥logo.
 *
 * üîπ FUNCIONALIDADES PRINCIPAIS:
 * - Estabelece chamada de v√≠deo e √°udio entre dois usu√°rios (psic√≥logo e paciente).
 * - Identifica o papel do participante (psic√≥logo ou paciente) e organiza a transcri√ß√£o conforme o papel.
 * - Usa o Web Audio API (`AnalyserNode`) para monitorar o volume de entrada e determinar se algu√©m est√° falando.
 * - Envia o ID do peer para uma API back-end para mapeamento com a sess√£o da sala.
 * - Exibe a transcri√ß√£o em tempo real usando o componente `<LiveTranscription />`.
 * - Permite ligar/desligar microfone e v√≠deo localmente.
 *
 * üîß USO DE HOOKS E REFER√äNCIAS:
 * - `useState`: Controla estados de interface e dados como v√≠deo, √°udio, transcri√ß√£o, ID do peer, etc.
 * - `useRef`: Armazena elementos de v√≠deo/√°udio e inst√¢ncias persistentes como `Peer` e `AudioContext`.
 * - `useEffect`: Inicializa o peer e define os manipuladores de chamada ao carregar o componente.
 * - `useParams`: Captura o `iddinamico` da URL para identificar a sala.
 *
 * üß† MONITORAMENTO DE MICROFONE:
 * - A fun√ß√£o `monitorMicrophone` usa `AnalyserNode` para ler o volume do microfone.
 * - Quando o volume √© alto o suficiente, assume que o usu√°rio est√° falando e dispara `handleTranscription`.
 * - O √°udio remoto √© automaticamente mutado para evitar eco durante a fala.

 * üìù TRANSCRI√á√ÉO:
 * - `handleTranscription` atualiza o estado com a transcri√ß√£o da conversa, separando psic√≥logo e paciente.
 * - A transcri√ß√£o √© exibida ao vivo para o psic√≥logo, e associada √† sala em quest√£o.

 * üìπ INTERFACE:
 * - V√≠deo principal: mostra o v√≠deo do outro participante.
 * - V√≠deo em miniatura: mostra o pr√≥prio v√≠deo.
 * - Controles flutuantes: ativar/desativar microfone, v√≠deo e encerrar chamada.
 *
 * üö® NOTAS:
 * - A parte de ligar/desligar microfone e v√≠deo ainda precisa ser implementada funcionalmente.
 * - Atualmente, a transcri√ß√£o √© feita apenas do lado do paciente (ou quem estiver configurado como `!isPsychologist`).
 * - A grava√ß√£o e transcri√ß√£o do √°udio do psic√≥logo diretamente do alto-falante ainda est√° em desenvolvimento.
 *
 * üß™ DEPEND√äNCIAS:
 * - `peerjs`: conex√£o WebRTC simplificada.
 * - `lucide-react`: √≠cones para a interface.
 * - `LiveTranscription`: componente personalizado para exibir e salvar transcri√ß√µes.

 * @returns JSX.Element - Interface completa da videochamada com transcri√ß√£o ao vivo.
 */


export default function PublicCallPage() {



/**
 * ID do peer remoto (opcionalmente utilizado para refer√™ncia futura).
 * Pode ser usado para reconectar ou registrar a chamada.
 */
const [remoteId, setRemoteId] = useState<string>("");

/**
 * Mensagem de status exibida durante a transcri√ß√£o (ex: "Aguardando transcri√ß√£o").
 */
const [msg, setMsg] = useState<string>("Aguardando transcri√ß√£o");

/**
 * Refer√™ncia ao contexto de √°udio utilizado para processar e analisar o som do microfone.
 */
const audioContextRef = useRef<AudioContext | null>(null);

/**
 * Refer√™ncia ao analisador de frequ√™ncia da Web Audio API.
 * Utilizado para monitorar o volume do microfone em tempo real.
 */
const analyserRef = useRef<AnalyserNode | null>(null);

/**
 * Fonte de √°udio conectada ao microfone local, usada como entrada para o analisador.
 */
const sourceRef = useRef<MediaStreamAudioSourceNode | null>(null);

/**
 * Refer√™ncia ao elemento de √°udio do participante remoto, utilizado para controlar o som recebido.
 */
const remoteAudioRef = useRef<HTMLAudioElement | null>(null);

/**
 * ID din√¢mico da sala de atendimento, extra√≠do da URL usando o App Router do Next.js.
 */
const { iddinamico } = useParams();

/**
 * ID √∫nico gerado para o peer atual ao se conectar ao servidor PeerJS.
 */
const [peerId, setPeerId] = useState<string>("");

/**
 * Indica se uma chamada de v√≠deo est√° ativa no momento.
 */
const [callActive, setCallActive] = useState<boolean>(false);

/**
 * Refer√™ncia ao objeto PeerJS que representa o peer local.
 */
const peerRef = useRef<Peer | null>(null);

/**
 * Refer√™ncia ao elemento de v√≠deo local (do pr√≥prio usu√°rio).
 */
const videoRef = useRef<HTMLVideoElement | null>(null);

/**
 * Refer√™ncia ao v√≠deo do participante remoto.
 */
const remoteVideoRef = useRef<HTMLVideoElement | null>(null);

/**
 * Representa a conex√£o de m√≠dia ativa com o outro participante da chamada.
 */
const currentCall = useRef<MediaConnection | null>(null);

/**
 * Estado que controla se o microfone local est√° ligado ou desligado.
 */
const [mic, setMic] = useState<boolean>(true);

/**
 * Estado que controla se a c√¢mera local est√° ligada ou desligada.
 */
const [video, setVideo] = useState<boolean>(true);

/**
 * Define se o usu√°rio atual √© o psic√≥logo.
 * Usado para rotular corretamente as falas na transcri√ß√£o.
 */
const [isPsychologist, setIsPsychologist] = useState<boolean>(true);

/**
 * Armazena a transcri√ß√£o da conversa realizada durante a chamada.
 */
const [transcription, setTranscription] = useState<string>("");


 

  /**
   * Fun√ß√£o para monitorar o volume do microfone.
   * 
   * - Cria um contexto de √°udio se ainda n√£o existir.
   * - Configura um analisador de frequ√™ncia para detectar atividade sonora.
   * - Conecta a fonte de √°udio do microfone ao analisador.
   * @param {MediaStream} stream - Fluxo de √°udio do microfone local.
   * @returns {void}
   */
  const monitorMicrophone = (stream: MediaStream) => {
    if (!audioContextRef.current) {
      audioContextRef.current = new AudioContext();
    }
    const audioContext = audioContextRef.current;
    const analyser = audioContext.createAnalyser();
    analyser.fftSize = 256; // Defini√ß√£o do tamanho da an√°lise de frequ√™ncia
    analyserRef.current = analyser;

    const source = audioContext.createMediaStreamSource(stream);
    source.connect(analyser);
    sourceRef.current = source;

    const dataArray = new Uint8Array(analyser.frequencyBinCount);

    const checkVolume = () => {
      analyser.getByteFrequencyData(dataArray);
      const volume = dataArray.reduce((a, b) => a + b) / dataArray.length; // Calcula o volume m√©dio

      if (remoteAudioRef.current) {
        remoteAudioRef.current.muted = volume > 10; // Se estiver falando, muta o alto-falante
      }
      handleTranscription(transcription, !isPsychologist);
      requestAnimationFrame(checkVolume);
    };

    checkVolume();
  };


  /**
 * Atualiza a transcri√ß√£o da conversa em tempo real, identificando o falante como psic√≥logo ou paciente.
 *
 * @param {string} text - Texto da fala capturada para ser adicionada √† transcri√ß√£o.
 * @param {boolean} isPsychologist - Indica se a fala √© do psic√≥logo (`true`) ou do paciente (`false`).
 */

  const handleTranscription = (text: string, isPsychologist: boolean) => {
    // Definindo quem est√° falando, psic√≥logo ou paciente
    const speaker = isPsychologist ? 'psicologo' : 'paciente';

    // Atualizando a transcri√ß√£o com o t√≠tulo correto
    setTranscription(prevTranscription => prevTranscription + `\n${speaker}: ${transcription}`);

  };



/**
 * Efeito que inicializa a conex√£o PeerJS para chamadas de v√≠deo.
 *
 * - Cria um novo `Peer` e obt√©m seu ID ao abrir a conex√£o.
 * - Envia o `peerId` gerado para a API associando-o ao `iddinamico` da URL.
 * - Escuta chamadas recebidas e responde com m√≠dia local (√°udio/v√≠deo).
 * - Conecta os fluxos de m√≠dia local e remota aos elementos de v√≠deo correspondentes.
 * - Inicia o monitoramento do microfone para controlar o √°udio remoto com base no volume.
 * - Limpa e destr√≥i o peer ao desmontar o componente.
 *
 * @dependency `iddinamico` - ID din√¢mico extra√≠do da URL, necess√°rio para associar o `peerId` na API.
 */

  useEffect(() => {
    if (!iddinamico) return; // Se n√£o tem ID na URL, n√£o faz nada

    const peer = new Peer(); // Cria um novo peer
    peerRef.current = peer;

    peer.on("open", async (id) => {
      setPeerId(id); // Define o ID do Peer

      try {
        // Envia o ID gerado para a API
        await fetch(`/api/save_peer?iddinamico=${iddinamico}`, {
          method: "POST",
          headers: { "Content-Type": "application/json" },
          body: JSON.stringify({ iddinamico, peerId: id }),
        });
      } catch (error) {
        console.error("Erro ao enviar peerId para a API:", error);
      }
    });

    // Responder chamadas do psic√≥logo
    peer.on("call", (call) => {
      navigator.mediaDevices.getUserMedia({ video: true, audio: true }).then((stream) => {
        if (videoRef.current) videoRef.current.srcObject = stream;
        call.answer(stream);
        setCallActive(true);
        currentCall.current = call;
        monitorMicrophone(stream);
        setMsg('Transcrevendo Chamada...');


        call.on("stream", (remoteStream) => {
          if (remoteVideoRef.current) remoteVideoRef.current.srcObject = remoteStream;

          if (remoteVideoRef.current) {
            remoteVideoRef.current.srcObject = remoteStream;
          }
        
          // Se quiser, tamb√©m pode criar um elemento <audio> separado
          if (remoteAudioRef.current) {
            remoteAudioRef.current.srcObject = remoteStream;
            remoteAudioRef.current.play();
          }

          
        });

        call.on("close", () => endCall());
      });
    });

    return () => peer.destroy(); // Limpa o peer ao desmontar
  }, [iddinamico]);




  /** 
   * Encerra a chamada de v√≠deo atual.
   *
   * - Limpa a mensagem de status.
   * - Fecha a conex√£o ativa.
   * - Para o fluxo de v√≠deo local.
   * - Para o fluxo de v√≠deo remoto.
   * 
   */
  const endCall = () => {
    setMsg('');
    if (currentCall.current) currentCall.current.close();
    if (videoRef.current?.srcObject) {
      (videoRef.current.srcObject as MediaStream).getTracks().forEach(track => track.stop());
      videoRef.current.srcObject = null;
    }
    if (remoteVideoRef.current?.srcObject) {
      (remoteVideoRef.current.srcObject as MediaStream).getTracks().forEach(track => track.stop());
      remoteVideoRef.current.srcObject = null;
    }
    setCallActive(false);
  };



  /**
 * Renderiza a interface da chamada de v√≠deo entre psic√≥logo e paciente.
 *
 * - Exibe o v√≠deo remoto (psic√≥logo) em tela cheia.
 * - Exibe o v√≠deo local (paciente) em miniatura no canto inferior esquerdo.
 * - Mostra a identifica√ß√£o de quem √© quem na chamada.
 * - Inclui um bot√£o "Sair" para encerrar a chamada.
 * - Exibe a transcri√ß√£o ao vivo da fala do paciente, sendo enviada ao psic√≥logo.
 * - Controles para ligar/desligar microfone e c√¢mera, com √≠cones que refletem o estado atual.
 *
 * @returns {JSX.Element} JSX do layout da chamada com controles e transcri√ß√£o.
 */

  return (
    <div className="relative w-screen h-screen bg-gradient-to-r from-blue-400 to-green-300">
      {/* V√≠deo do Paciente - Ocupa 100% da tela */}
      {/*  <video ref={videoRef} autoPlay playsInline className="absolute inset-0 w-full h-full object-cover" /> */}
      <video ref={remoteVideoRef} autoPlay playsInline className="absolute inset-0 w-full h-full object-cover" />
      <div className="absolute top-2 left-2 text-blue-800 p-2 font-semibold text-sm">
        Psicologo
      </div>


      {/* V√≠deo do Psic√≥logo - Menor no canto inferior esquerdo */}
      <div className="absolute bottom-4 left-4 w-1/4 h-auto">
        {/* <video ref={remoteVideoRef} autoPlay playsInline className="w-full h-auto bg-black object-cover border-2 border-indigo-500" /> */}
        <video
         ref={videoRef}
          autoPlay
           playsInline
           className="w-full h-auto bg-black object-cover border-2 border-indigo-500" 
           muted={true}
           />
        <div className="absolute top-2 left-2 bg-black bg-opacity-50 text-white p-2 font-semibold text-sm">
          voc√™
        </div>
      </div>

      {/* Bot√£o de Sair - Ajustado para ficar acima do v√≠deo */}
      {callActive && (
        <div className="absolute top-4 right-4">
          <button
            onClick={endCall}
            className="py-3 px-6 text-white bg-gradient-to-r from-red-600 to-red-400 shadow-lg hover:bg-red-500 focus:outline-none focus:ring-2 focus:ring-red-500 transition duration-200"
          >
            Sair
          </button>
        </div>
      )}
      {/* nessa vers√£o vamos buscar a transcri√ß√£o do paciente e enviar para o psic√≥logo, 
      mas estamos trabalhando para conseguir buscar a trasncri√ßao diretor do auto falante do psicologo */}

      <div className=" absolute  bottom-[40%] right-4 w-auto max-w-[30%]">
        <LiveTranscription
          usuario={'Paciente'}
          mensagem={transcription}
          sala={iddinamico as string}

        />
      </div>


      <div className="absolute bottom-6 left-[50%] transform -translate-x-1/2 flex space-x-6 p-4 rounded-xl">

        {/* bot√£o para desativar o microfone */}
        <button
          className="p-3 bg-gray-700 text-white rounded-full shadow-md hover:bg-gray-800 transition"
          onClick={() => {
            setMic(!mic)
            if (mic) {
              //LIGAR MICROFONE
            } else {
              //DESLIGAR MICROFONE
            }
          }}
        >
          {mic ? <Mic size={12} /> : <MicOff size={12} />}
        </button>


        <button
          className="p-3 bg-gray-700 text-white rounded-full shadow-md hover:bg-gray-800 transition"
          onClick={() => {
            setVideo(!video)
            if (video) {
              //LIGAR VIDEO
            } else {
              //DESLIGAR VIDEO
            }
          }}
        >
          {video ? <Video size={12} /> : <VideoOff size={12} />}
        </button>

        <button
          className="p-3 bg-red-600 text-white rounded-full shadow-md hover:bg-red-700 transition"
          onClick={endCall}
        >
          <LogOut size={12} />
        </button>
      </div>

    </div>



  );
}
