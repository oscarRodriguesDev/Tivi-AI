"use client";

/**
 * Importa√ß√µes de depend√™ncias e utilit√°rios usados no componente de transcri√ß√£o:
 *
 * - `useState`, `useEffect` (React): Hooks utilizados para gerenciar o estado e efeitos colaterais do componente.
 * - `jsPDF`: Biblioteca para gera√ß√£o de arquivos PDF diretamente no cliente.
 * - `FaBrain`, `FaEraser`, `FaFilePdf`, `FaStop` (react-icons/fa): √çcones utilizados na interface para representar a√ß√µes como an√°lise, limpar, exportar PDF e parar transcri√ß√£o.
 * - `RiPlayList2Fill` (react-icons/ri): √çcone utilizado para representar o bot√£o de iniciar transcri√ß√£o.
 * - `useParams` (next/navigation): Hook do Next.js App Router utilizado para capturar par√¢metros da URL, como o identificador da sala.
 */

import { useState, useEffect } from "react";
import jsPDF from "jspdf";
import { FaBrain, FaEraser, FaFilePdf, FaStop } from "react-icons/fa";
import { RiPlayList2Fill } from "react-icons/ri";
import { useParams } from "next/navigation";
import { showErrorMessage, showInfoMessage} from "@/app/util/messages";


/**
 * Interface para definir as propriedades do componente `LiveTranscription`.
 *
 * - `mensagem`: Mensagem a ser exibida no componente.
 * - `usuario`: Nome do usu√°rio que est√° falando.
 * - `sala`: Identificador da sala de reuni√£o.
 * 
 * @interface LiveTranscriptionProps
 * @property {string} mensagem - Mensagem a ser exibida no componente.
 * @property {string} usuario - Nome do usu√°rio que est√° falando.
 * @property {string} sala - Identificador da sala de reuni√£o.
 */
interface LiveTranscriptionProps {
  mensagem: string;
  usuario: string;
  sala: string
}



/**
 * Componente `LiveTranscription`
 * 
 * Este componente √© respons√°vel por realizar a transcri√ß√£o de voz em tempo real, 
 * associando-a a uma sala virtual para sess√µes entre psic√≥logos e pacientes.
 * 
 * Recursos principais:
 * 
 * - üéôÔ∏è **Reconhecimento de voz (SpeechRecognition)**:
 *   Usa a API nativa de reconhecimento de fala do navegador (`SpeechRecognition` ou `webkitSpeechRecognition`)
 *   para transcrever a fala do usu√°rio automaticamente em tempo real.
 * 
 * - üí¨ **Armazenamento e recupera√ß√£o de transcri√ß√µes**:
 *   As transcri√ß√µes s√£o salvas em um backend via API REST e recuperadas conforme necess√°rio, 
 *   garantindo persist√™ncia e sincroniza√ß√£o dos dados da sess√£o.
 * 
 * - üß† **An√°lise com IA (ChatGPT)**:
 *   Envia a transcri√ß√£o para uma rota que responde com insights, an√°lises e observa√ß√µes que auxiliam o psic√≥logo.
 * 
 * - üìÑ **Exporta√ß√£o para PDF**:
 *   Gera um arquivo `.pdf` da transcri√ß√£o e da an√°lise com layout formatado para impress√£o ou arquivamento.
 * 
 * - üéõÔ∏è **Controles manuais e autom√°ticos**:
 *   O usu√°rio pode iniciar/parar a transcri√ß√£o manualmente ou permitir que o sistema inicie automaticamente 
 *   ap√≥s o aceite da permiss√£o de microfone.
 * 
 * - ‚ö†Ô∏è **Tratamento de erros e notifica√ß√µes**:
 *   Exibe mensagens de erro caso o navegador n√£o suporte o recurso ou se houver falhas no processo.
 * 
 * Props esperadas:
 * 
 * @component
 * @param {Object} props
 * @param {string} props.usuario - Nome ou identificador de quem est√° falando, usado para prefixar as falas.
 * @param {string} props.mensagem - Mensagem inicial ou conte√∫do relevante para an√°lise (n√£o utilizado diretamente aqui).
 * @param {string} props.sala - Identificador da sala usada para salvar/recuperar transcri√ß√µes associadas.
 * 
 * @returns {JSX.Element} Interface com os controles de transcri√ß√£o, exibi√ß√£o do conte√∫do e a√ß√µes como salvar e analisar.
 * 
 * @example
 * <LiveTranscription usuario="Paciente" mensagem="" sala="abc123" />
 */


export default function LiveTranscription({ usuario, mensagem, sala }: LiveTranscriptionProps) {

  /** 
   * Estado que armazena a transcri√ß√£o atual da fala reconhecida.
   * Atualizada toda vez que uma nova fala √© reconhecida como final.
   */
  const [transcription, setTranscription] = useState<string>("");

  /**
   * Inst√¢ncia da API de reconhecimento de voz (`SpeechRecognition`).
   * Inicializada ap√≥s verifica√ß√£o de suporte e permiss√£o ao microfone.
   */
  const [recognition, setRecognition] = useState<any>(null);

  /**
   * Indica se o reconhecimento de voz est√° atualmente ativo (em escuta).
   * Usado para alternar o bot√£o de iniciar/parar.
   */
  const [listening, setListening] = useState<boolean>(false);

  /**
   * Armazena mensagens de erro relacionadas √† transcri√ß√£o,
   * como falta de suporte no navegador ou falha de permiss√£o.
   */
  const [error, setError] = useState<string>("");

  /**
   * T√≠tulo da sess√£o ou da transcri√ß√£o. Pode ser definido externamente ou dinamicamente.
   * Exibido no topo da interface de transcri√ß√£o.
   */
  const [titulo, setTitulo] = useState<string>("");

  /**
   * Armazena a resposta gerada pela an√°lise da conversa usando IA.
   * Exibida ao psic√≥logo ou exportada no PDF.
   */
  const [analise, setAnalise] = useState<string>("nenhuma analise");

  /**
   * Indica se o sistema est√° "ligado" para iniciar a transcri√ß√£o automaticamente.
   * √ötil para controlar o modo de escuta autom√°tica.
   */
  const [ligado, setLigado] = useState<boolean>(false);
  //usar essa variavel pra controlar quando vai transcrever







  /**
   * Busca as mensagens transcritas do servidor com base na sala atual.
   * 
   * - Realiza uma requisi√ß√£o GET para a rota `/api/message`, passando o identificador da sala.
   * - Se a resposta contiver uma transcri√ß√£o (`transcript`), atualiza o estado `transcription` apenas se o conte√∫do for diferente do atual.
   * - Em caso de erro na requisi√ß√£o, atualiza o estado de erro com uma mensagem descritiva.
   */
  const fetchMessages = async () => {
    try {
      const response = await fetch(`/api/message/?sala=${sala}`, { method: 'GET' });

      if (!response.ok) {
        throw new Error("Erro ao recuperar mensagens.");
      }

      const data = await response.json();

      if (data.transcript) {
        const cleanedTranscript = data.transcript;

        // S√≥ atualiza se o texto limpo for realmente novo
        if (cleanedTranscript !== transcription) {
          setTranscription(cleanedTranscript);
        }
      }
    } catch (error) {
      setError(`Erro ao carregar mensagens: ${error}`);
    }
  };




  /**
   * Salva uma transcri√ß√£o no servidor associada a uma sala espec√≠fica.
   * 
   * - Envia uma requisi√ß√£o POST para a rota `/api/message` com o conte√∫do da transcri√ß√£o e o identificador da sala.
   * - Em caso de sucesso, limpa a transcri√ß√£o atual e busca novamente todas as mensagens.
   * - Em caso de erro, atualiza o estado de erro com a mensagem correspondente.
   * 
   * @param {string} transcript - Texto transcrito a ser salvo no servidor.
   */
  const saveMessage = async (transcript: string) => {
    try {
      const response = await fetch('/api/message', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({ sala, transcript }),
      });

      if (!response.ok) {
        throw new Error("Erro ao salvar a mensagem.");
      }

      // Ap√≥s salvar a mensagem, busca as mensagens novamente
      handleClearTranscription()
      await fetchMessages();
    } catch (error) {
      setError(`Erro ao salvar a mensagem: ${error}`);
    }
  };



  /**
   * Inicia o reconhecimento de voz.
   * 
   * - Verifica se a inst√¢ncia de reconhecimento est√° dispon√≠vel.
   * - Se dispon√≠vel, inicia a escuta e atualiza o estado `listening` para `true`.
   * - Caso contr√°rio, exibe um erro no
   */
  const handleStartListening = () => {
    if (!recognition) {
      showErrorMessage("Reconhecimento de voz n√£o foi inicializado corretamente.");
      return;
    }
    setListening(true);
    recognition.start();
  };



  /**
   * Interrompe o reconhecimento de voz.
   * 
   * - Verifica se a inst√¢ncia de reconhecimento existe.
   * - Atualiza o estado `listening` para `false`.
   * - Chama o m√©todo `stop()` para encerrar a escuta de voz.
   */
  const handleStopListening = () => {
    if (!recognition) return;
    setListening(false);
    recognition.stop();
  };


  /**
   * Limpa o conte√∫do da transcri√ß√£o atual.
   * 
   * - Reseta o estado `transcription` para uma string vazia.
   */
  const handleClearTranscription = () => {
    setTranscription("");
  };




  /**
   * Salva a transcri√ß√£o atual em um arquivo PDF.
   * 
   * - Cria um novo documento PDF com orienta√ß√£o vertical.
   * - Define o formato de p√°gina como A4.
   * - Calcula o espa√ßamento entre linhas e a largura m√°xima de cada p√°gina. 
   * 
   * @returns {void}
   * 
   */
  function handleSavePDF(): void {
    const doc = new jsPDF({
      orientation: "portrait",
      unit: "mm",
      format: "a4",
    });

    const marginLeft = 10;
    const marginTop = 10;
    const pageWidth = doc.internal.pageSize.getWidth();
    const pageHeight = doc.internal.pageSize.getHeight();
    const lineHeight = 7; // Espa√ßamento entre linhas
    const maxWidth = pageWidth - marginLeft * 2;

    let yPos = marginTop;

    const wrapText = (text: string): string[] => {
      return doc.splitTextToSize(text, maxWidth);
    };

    const addTextToPDF = (text: string): void => {
      const lines = wrapText(text);
      lines.forEach((line) => {
        if (yPos + lineHeight > pageHeight - marginTop) {
          doc.addPage();
          yPos = marginTop;
        }
        doc.text(line, marginLeft, yPos);
        yPos += lineHeight;
      });
    };

    addTextToPDF(transcription);
    yPos += lineHeight * 2;

    addTextToPDF("An√°lise detalhada da conversa:\n");
    yPos += lineHeight;

    addTextToPDF(analise);

    doc.save("transcricao.pdf");
  }


  /**
   * Busca insights sobre a transcri√ß√£o atual usando a API de an√°lise com IA.
   * 
   * - Envia uma requisi√ß√£o POST para a rota `/api/psicochat` com a mensagem transcrita.
   * - Recebe a resposta da API e atualiza o estado `analise` com o conte√∫do da resposta.
   *    
   * @param {string} mensagem - Mensagem transcrita a ser enviada para a API de an√°lise.
   * 
   * @returns {Promise<string>} Resposta da API de an√°lise.
   */

  const handleGetInsights = async (mensagem: string) => {
    try {
      const response = await fetch('/api/psicochat', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({ message: mensagem }),
      });

      if (!response.ok) {
        throw new Error(`Erro na requisi√ß√£o: ${response.statusText}`);
      }

      const data = await response.json();
      const respostaGPT = data.response || "Nenhuma resposta gerada.";
      setAnalise(respostaGPT);
      return respostaGPT;


    } catch (error) {
      showErrorMessage("Erro ao buscar insights:" + error);
      return "Erro ao obter resposta.";
    }
  };




  /**
   * Inicializa o reconhecimento de voz automaticamnte.
   * 
   * - Verifica se o navegador suporta a API `SpeechRecognition`.
   * - Configura o reconhecimento cont√≠nuo de fala em portugu√™s (PT-BR).
   * - Escuta os resultados finais e atualiza a transcri√ß√£o com o nome do usu√°rio.
   * 
   * @returns {void}
   */

  useEffect(() => {
    if (typeof window === "undefined") return;

    const SpeechRecognition =
      (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition;

    if (!SpeechRecognition) {
      setError("Seu navegador n√£o suporta reconhecimento de voz.");
      return;
    }

    // Fun√ß√£o para pedir permiss√£o e iniciar a transcri√ß√£o
    const requestMicrophonePermission = async () => {
      try {
        // Tentamos acessar o microfone. O prompt de permiss√£o √© exibido automaticamente.
        await navigator.mediaDevices.getUserMedia({ audio: true });

        const confirmTranscription = window.confirm("Deseja iniciar a transcri√ß√£o autom√°tica da sess√£o?");
        if (!confirmTranscription) {
          setError("Transcri√ß√£o n√£o iniciada - usu√°rio n√£o autorizou");
          return;
        }

        const recognitionInstance = new SpeechRecognition();
        recognitionInstance.continuous = true;
        recognitionInstance.interimResults = false;
        recognitionInstance.lang = "PT-BR";

        recognitionInstance.onresult = (event: SpeechRecognitionEvent) => {
          let transcript = "";

          for (let i = event.resultIndex; i < event.results.length; i++) {
            const result = event.results[i];
            if (result.isFinal) {
              transcript = result[0].transcript + "\n";
            }
          }

          if (transcript.trim()) {
            setTranscription((prev) => `${usuario}: ${transcript}`);
            saveMessage(`${usuario}: ${transcript}`); // Salvar transcri√ß√£o na API
          }
        };

        recognitionInstance.onerror = (event: SpeechRecognitionErrorEvent) => {

          //deixar pra verificar essa quest√£o da trascri√ßao,tem que pensar direito a respeito
          showInfoMessage('A transcri√ß√£o foi desativada')


        };

        setRecognition(recognitionInstance);

        recognitionInstance.start();
      } catch (err) {
        setError("Permiss√£o para usar o microfone n√£o concedida.");
      }
    };

    // Dispara a solicita√ß√£o de permiss√£o
    requestMicrophonePermission();



    // Cleanup
    return () => {
      if (recognition) {
        recognition.stop();
      }
    };
  }, []);



  /**
 * Renderiza a interface de transcri√ß√£o ao vivo.
 *
 * A interface cont√©m:
 * - T√≠tulo da transcri√ß√£o.
 * - Mensagem de erro (se houver).
 * - √Årea de exibi√ß√£o da transcri√ß√£o, ou mensagem de espera.
 * - Bot√µes de controle:
 *   - Limpar transcri√ß√£o (√≠cone de borracha).
 *   - Salvar como PDF (√≠cone de PDF).
 *   - Obter insights (√≠cone de c√©rebro).
 *   - Iniciar ou parar transcri√ß√£o de voz (√≠cones de play/stop).
 *
 * Estilos responsivos e rolagem inclusos para melhor usabilidade.
 */


  return (
    <div className="">


      <div className=" w-96 ml-10 pb-4 rounded-lg p-4 overflow-y-auto h-full">
        <h1 className="text-lg font-semibold text-center mb-2 text-white">{titulo}</h1>

        {error && <p className="text-red-500 text-center mb-4">{error}</p>}



        <div className="  flex-1 overflow-y-auto p-2 rounded-md text-sm text-white  max-h-[60vh]">
          {transcription ? (
            <p className="whitespace-pre-wrap">{transcription}</p>
          ) : (
            <p className="text-gray-800 text-center">{'Aguardando transcri√ß√£o...'}</p>
          )}
        </div>


      </div>



      <div className="absolute top-5  mt-auto mb-auto w-24">


        <div className="pb-1">


          <button
            onClick={handleClearTranscription}
            className="bg-blue-500 text-white px-4 py-2 ml-1 rounded-md hover:bg-blue-600 transition"
            title="Limpar Transcri√ß√£o"
          >
            <FaEraser size={10} />
          </button>
          <button
            onClick={handleSavePDF}
            className="bg-yellow-500 text-white px-4 py-2 ml-1 rounded-md hover:bg-yellow-600 transition"
            title="Salvar PDF"
          >
            <FaFilePdf size={10} />
          </button>
        </div>
        <button
          onClick={() => handleGetInsights(transcription)}
          className="bg-yellow-500 text-white px-4 py-2 rounded-md ml-1 hover:bg-yellow-600 transition"
          title="An√°lise"
        >
          <FaBrain size={10} />
        </button>


        {!listening ? (
          <button
            onClick={handleStartListening}
            className="bg-red-600 text-white px-4 py-2 ml-1 rounded-md hover:bg-red-500 transition"
            title="Iniciar Transcri√ß√£o"
          >
            <RiPlayList2Fill size={10} />
          </button>
        ) : (
          <button
            onClick={handleStopListening}
            className="bg-red-600 text-white px-4 py-2 rounded-md hover:bg-red-500 transition"
            title="Parar Transcri√ß√£o"
          >
            <FaStop size={10} />

          </button>
        )}

      </div>
      *



    </div>
  );
}