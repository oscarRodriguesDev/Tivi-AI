'use client';

/**
 * Importa√ß√µes necess√°rias para a p√°gina de videochamada com transcri√ß√£o ao vivo:
 * 
 * - `useEffect`, `useRef`, `useState`: Hooks do React usados para controlar estado, efeitos colaterais e refer√™ncias a elementos DOM.
 * - `useParams`, `useSearchParams`: Hooks do Next.js App Router para acessar par√¢metros da URL.
 * - `uuidv4`: Gera√ß√£o de IDs √∫nicos, utilizado para criar o ID do peer na rede P2P.
 * - `Peer`, `MediaConnection`: Biblioteca PeerJS para criar e gerenciar conex√µes WebRTC ponto-a-ponto (P2P).
 * - `LiveTranscription`: Componente customizado para exibir a transcri√ß√£o ao vivo da conversa.
 * - `HeadPage`: Componente para configurar o cabe√ßalho da p√°gina (t√≠tulo e √≠cone).
 * - `FaVideo`, `FcVideoCall`, `FcEndCall`: √çcones SVG utilizados nos bot√µes da interface para indicar a√ß√µes de v√≠deo.
 */

import { useEffect, useRef, useState } from "react";
import { useParams, useSearchParams } from "next/navigation";
import { v4 as uuidv4 } from "uuid";
import Peer, { MediaConnection } from "peerjs";
import LiveTranscription from "../../dating/components/transcriptionPSC";
import HeadPage from "@/app/(private-access)/components/headPage";
import { FaVideo } from "react-icons/fa";
import { FcVideoCall, FcEndCall } from "react-icons/fc";
import { showErrorMessage } from "@/app/util/messages";


/**
 * P√°gina principal da sala de reuni√£o com videochamada entre psic√≥logo e paciente,
 * integrando transcri√ß√£o em tempo real e controle de fluxo da chamada.
 * 
 * Funcionalidades principais:
 * 
 * - Inicializa uma inst√¢ncia do PeerJS (tecnologia P2P/WebRTC) para permitir conex√£o entre dois usu√°rios (psic√≥logo e paciente).
 * - Gerencia chamadas de v√≠deo com √°udio e v√≠deo via WebRTC, permitindo:
 *    - Iniciar chamada
 *    - Receber chamadas
 *    - Encerrar chamadas
 * - Exibe o v√≠deo local (psic√≥logo) e remoto (paciente) em tela.
 * - Monitora o volume do microfone para detectar fala e controlar o √°udio remoto (evita eco).
 * - Realiza transcri√ß√£o de falas, separando entre "psic√≥logo" e "paciente".
 * - Renderiza transcri√ß√£o unificada usando o componente `<LiveTranscription />`.
 * 
 * Hooks usados:
 * - `useState`: controla o estado da chamada, transcri√ß√£o, IDs, etc.
 * - `useEffect`: inicializa o PeerJS e escuta chamadas recebidas.
 * - `useRef`: mant√©m refer√™ncias para elementos DOM (v√≠deos, √°udio) e inst√¢ncias (PeerJS, MediaConnection).
 * - `useParams` e `useSearchParams`: obt√©m identificadores da URL, usados para vincular os participantes da reuni√£o.
 * 
 * Layout:
 * - Usa Tailwind CSS para posicionamento e responsividade.
 * - Interface com bot√µes de controle da chamada (iniciar e encerrar).
 * - V√≠deo principal para o paciente, miniatura sobreposta para o psic√≥logo.
 * - √Årea lateral exibe a transcri√ß√£o das falas em tempo real.
 * 
 * üîí Importante:
 * - Os pap√©is (psic√≥logo ou paciente) s√£o determinados pelo par√¢metro `iddinamico` da URL.
 * - Toda transcri√ß√£o gerada √© mantida localmente no estado e enviada para backend separadamente (n√£o nesta fun√ß√£o).
 */


export default function Home() {

   /**
   * ID gerado automaticamente pelo PeerJS para identificar este cliente (psic√≥logo ou paciente).
   */
   const [peerId, setPeerId] = useState<string>("");

   /**
    * ID do outro participante da chamada (usado para conectar a outra ponta).
    */
   const [remoteId, setRemoteId] = useState<string>("");
 
   /**
    * Mensagem de status ou transcri√ß√£o exibida na interface.
    */
   const [msg, setMsg] = useState<string>("Aguardando transcri√ß√£o");
 
   /**
    * Booleano que indica se uma chamada est√° atualmente ativa.
    */
   const [callActive, setCallActive] = useState<boolean>(false);
 
   /**
    * Refer√™ncia √† inst√¢ncia do PeerJS utilizada para chamadas WebRTC P2P.
    */
   const peerRef = useRef<Peer | null>(null);
 
   /**
    * Refer√™ncia ao elemento de v√≠deo local (psic√≥logo).
    */
   const videoRef = useRef<HTMLVideoElement | null>(null);
 
   /**
    * Refer√™ncia ao elemento de v√≠deo remoto (paciente).
    */
   const remoteVideoRef = useRef<HTMLVideoElement | null>(null);
 
   /**
    * Refer√™ncia √† chamada ativa do tipo `MediaConnection`.
    */
   const currentCall = useRef<MediaConnection | null>(null);
 
   /**
    * Refer√™ncia ao contexto de √°udio Web Audio API, usado para an√°lise de volume/microfone.
    */
   const audioContextRef = useRef<AudioContext | null>(null);
 
   /**
    * Refer√™ncia ao analisador de frequ√™ncia, usado para detectar atividade de voz.
    */
   const analyserRef = useRef<AnalyserNode | null>(null);
 
   /**
    * Fonte de √°udio vinculada ao microfone do usu√°rio.
    */
   const sourceRef = useRef<MediaStreamAudioSourceNode | null>(null);
 
   /**
    * Refer√™ncia a um elemento de √°udio que representa o som remoto (usado para mutar dependendo do volume).
    */
   const remoteAudioRef = useRef<HTMLAudioElement | null>(null);
 
   /**
    * Par√¢metros din√¢micos da URL obtidos via roteamento do Next.js.
    * Exemplo: /sala/123 ‚Üí params.idpaciente = "123"
    */
   const params = useParams();
 
   /**
    * Par√¢metros de consulta (query string) da URL.
    * Exemplo: /sala?idpaciente=123&iddinamico=abc ‚Üí idpaciente = "123", iddinamico = "abc"
    */
   const searchParams = useSearchParams();
 
   /**
    * ID din√¢mico do paciente extra√≠do da URL (rota din√¢mica).
    */
   const iddinamico = params.idpaciente;
 
   /**
    * ID do paciente extra√≠do da query string (?iddinamico=).
    */
   const idpaciente = searchParams.get('iddinamico');
 
   /**
    * Booleano que identifica se o usu√°rio atual √© o psic√≥logo (padr√£o: true).
    */
   const [isPsychologist, setIsPsychologist] = useState<boolean>(true);
 
   /**
    * Transcri√ß√£o unificada contendo todas as falas da conversa (psic√≥logo e paciente).
    */
   const [transcription, setTranscription] = useState<string>("");
 



  /**
 * Inicia a an√°lise em tempo real do volume captado pelo microfone local,
 * utilizando a Web Audio API. O volume √© usado para identificar quando
 * o usu√°rio est√° falando e, se for o paciente, silenciar o √°udio remoto
 * para evitar eco durante a transcri√ß√£o.
 *
 * @param {MediaStream} stream - O fluxo de m√≠dia local contendo o √°udio do microfone.
 *
 * - Cria um `AudioContext` se ainda n√£o existir.
 * - Conecta o microfone a um `AnalyserNode` para an√°lise de frequ√™ncia.
 * - Verifica continuamente o volume e ajusta o `muted` do `remoteAudioRef` se necess√°rio.
 * - Chama `handleTranscription('text', isPsychologist)` em cada frame de an√°lise.
 *
 * ‚ö†Ô∏è Nota: o texto `'text'` passado para `handleTranscription` √© fixo neste trecho
 * e pode ser substitu√≠do por transcri√ß√£o real futuramente.
 */
  const monitorMicrophone = (stream: MediaStream) => {
    if (!audioContextRef.current) {
      audioContextRef.current = new AudioContext();
    }

    const audioContext = audioContextRef.current;
    const analyser = audioContext.createAnalyser();
    analyser.fftSize = 256; // Defini√ß√£o do tamanho da an√°lise de frequ√™ncia
    analyserRef.current = analyser;

    const source = audioContext.createMediaStreamSource(stream);
    source.connect(analyser);
    sourceRef.current = source;

    const dataArray = new Uint8Array(analyser.frequencyBinCount);

    const checkVolume = () => {
      analyser.getByteFrequencyData(dataArray);
      const volume = dataArray.reduce((a, b) => a + b) / dataArray.length; // Calcula o volume m√©dio

     /*  if (remoteAudioRef.current) {
        remoteAudioRef.current.muted = volume > 10; // Se estiver falando, muta o alto-falante
      } */
      handleTranscription('text', isPsychologist);
      requestAnimationFrame(checkVolume);
    };

    checkVolume();
  };


  useEffect(() => {
    if (!iddinamico) {
      return;
    }

    const peer = new Peer(uuidv4());
    peerRef.current = peer;
  
    peer.on("open", (id) => setPeerId(id));
    peer.on("call", (call) => {
      // Primeiro tenta com v√≠deo e √°udio
      navigator.mediaDevices.getUserMedia({ video: true, audio: true })
        .catch((error) => {
          showErrorMessage("V√≠deo bloqueado, tentando somente √°udio:", error);
          return navigator.mediaDevices.getUserMedia({ video: false, audio: true });
        })
        .then((stream) => {
          if (!stream) {
            throw new Error("Sem acesso ao microfone e c√¢mera");
          }
    
          if (videoRef.current && stream.getVideoTracks().length > 0) {
            videoRef.current.srcObject = stream;
          }
    
          call.answer(stream);
          setCallActive(true);
          currentCall.current = call;
          monitorMicrophone(stream);
          setMsg('Transcrevendo Chamada...');
    
          call.on("stream", (remoteStream) => {
            if (remoteVideoRef.current && remoteStream.getVideoTracks().length > 0) {
              remoteVideoRef.current.srcObject = remoteStream;
            }
            if (remoteAudioRef.current) {
              remoteAudioRef.current.srcObject = remoteStream;
              remoteAudioRef.current.play();
            }
          });
    
          call.on("close", () => endCall());
        })
        .catch((finalError) => {
          showErrorMessage("N√£o foi poss√≠vel acessar o microfone. Verifique as permiss√µes do navegador.");
        });
    });
    

    return () => {
      peer.destroy(); // Limpa inst√¢ncia ao desmontar
    };
  }, [iddinamico]);
  

/**
 * Inicia uma chamada de v√≠deo com o peer remoto utilizando PeerJS.
 *
 * - Define a mensagem de status como "Transcrevendo Chamada...".
 * - Define o `remoteId` com base no ID do paciente extra√≠do da URL.
 * - Solicita permiss√£o para acessar o microfone e a c√¢mera do dispositivo.
 * - Configura o stream local no `videoRef`.
 * - Realiza a chamada ao peer remoto utilizando o `remoteId` e envia o stream.
 * - Ao receber o stream remoto, define no `remoteVideoRef`.
 * testar
 *
 * ‚ö†Ô∏è Se `remoteId` ou `peerRef` n√£o estiverem definidos, a fun√ß√£o retorna sem executar.
 */
  const callPeer = () => {
    setMsg('Transcrevendo Chamada...');
    setRemoteId(idpaciente as string);

    if (!remoteId || !peerRef.current) return;

    navigator.mediaDevices.getUserMedia({
      video: true,
      audio: {
        echoCancellation: true, // Reduz eco do pr√≥prio microfone
        noiseSuppression: true, // Ajuda a reduzir ru√≠dos
        autoGainControl: false  // Evita que o volume mude sozinho
      }
    }).then((stream) => {
      if (videoRef.current) videoRef.current.srcObject = stream;

      const call = peerRef.current?.call(remoteId, stream);
      call?.on("stream", (remoteStream) => {
        if (remoteVideoRef.current) remoteVideoRef.current.srcObject = remoteStream;
      });
    });
  };




/**
 * Encerra a chamada de v√≠deo ativa e libera os recursos de m√≠dia.
 *
 * - Limpa a mensagem de status da interface.
 * - Fecha a conex√£o da chamada ativa, se existir.
 * - Interrompe todas as trilhas de m√≠dia (√°udio e v√≠deo) da c√¢mera local e do v√≠deo remoto.
 * - Remove os objetos de m√≠dia das refer√™ncias de v√≠deo (`videoRef` e `remoteVideoRef`).
 * - Atualiza o estado para indicar que a chamada n√£o est√° mais ativa.
 */

  const endCall = () => {
    setMsg('');

    if (currentCall.current) {
      currentCall.current.close();
    }
    if (videoRef.current?.srcObject) {
      (videoRef.current.srcObject as MediaStream).getTracks().forEach(track => track.stop());
      videoRef.current.srcObject = null;
    }
    if (remoteVideoRef.current?.srcObject) {
      (remoteVideoRef.current.srcObject as MediaStream).getTracks().forEach(track => track.stop());
      remoteVideoRef.current.srcObject = null;
    }
    setCallActive(false);
   
  };



/**
 * Atualiza o estado de transcri√ß√£o com a fala do interlocutor atual.
 *
 * @param text - Texto da fala transcrita.
 * @param isPsychologist - Indica se quem falou foi o psic√≥logo (`true`) ou o paciente (`false`).
 *
 * A transcri√ß√£o √© armazenada de forma cont√≠nua, com prefixo identificando o interlocutor.
 */

  const handleTranscription = (text: string, isPsychologist: boolean) => {
    const speaker = isPsychologist ? 'psicologo' : 'paciente';
    setTranscription(prevTranscription => prevTranscription + `\n${speaker}: ${text}`);
  };

/**
 * Componente de interface da sala de reuni√£o por v√≠deo.
 *
 * Exibe dois v√≠deos: um principal para o paciente e um menor sobreposto para o psic√≥logo.
 * Cont√©m bot√µes para iniciar e encerrar chamadas, al√©m de uma √°rea para transcri√ß√£o ao vivo da conversa.
 *
 * @returns JSX.Element - Estrutura visual da sala de reuni√£o.
 *
 * Elementos:
 * - `<video ref={remoteVideoRef}>`: V√≠deo do paciente (ocupando tela cheia).
 * - `<video ref={videoRef}>`: V√≠deo do psic√≥logo (em tamanho reduzido, sobreposto).
 * - Bot√µes:
 *   - `callPeer`: Inicia a chamada.
 *   - `endCall`: Encerra a chamada.
 * - `LiveTranscription`: Componente de transcri√ß√£o em tempo real unificada da conversa.
 */


  return (

    <>

      <HeadPage title='Sala de Reuni√£o' icon={<FaVideo size={20} />} />

      <div className=" flex  flex-col items-center justify-center w-[98%] h-[83vh] -my-5 bg-gradient-to-r from-blue-400 to-green-300 text-black p-8">

        <div className="relative w-full h-full">
          {/* video do paciente */}
          <video
            ref={remoteVideoRef}
            autoPlay
            playsInline
            className="absolute inset-0 w-full h-full bg-gray-300 border-4 border-indigo-500 object-cover"
          />
          <div className="absolute top-1 left-2 bg-black bg-opacity-50 text-white p-2 rounded-md font-semibold text-sm">
            Paciente
          </div>
        </div>

        {/* video pscologo */}
        <div className="absolute top-[63%] left-[20%] w-60 h-60">
          <video ref={videoRef}
            autoPlay
            playsInline
            className="w-full h-full bg-gray-600 rounded-lg shadow-lg z-50"
            muted={true}
          />
          <div className="absolute top-2 left-2 bg-black bg-opacity-50 text-white p-2 rounded-md font-semibold text-sm z-50">Psicologo</div>
        </div>


        <div className="absolute top-[90%] left-[55%] flex justify-center pt-8 gap-6 mb-8">
          <button
            onClick={callPeer}
            className="bg-blue-200 rounded-full p-1"
          >
            <FcVideoCall size={24} />

          </button>

          <button
            onClick={endCall}
            className="bg-blue-200 rounded-full p-1"
          >
            <FcEndCall size={24} />
          </button>


        </div>

        {/* Transcri√ß√£o unificada */}
         <div className="absolute top-[20%] left-[70%] flex justify-center pt-8 gap-6 mb-8">
        <LiveTranscription 
         usuario={'Psicologo'}
         mensagem={transcription} // A transcri√ß√£o agora √© unificada         
         sala={iddinamico as string}
        />
      </div> 
      </div>
    </>

  );
}